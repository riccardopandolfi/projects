{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96aa193b",
   "metadata": {},
   "source": [
    "# Web and Social Media Analytics Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e33128",
   "metadata": {},
   "source": [
    "### Import the data and the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b4178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/riccardopandolfi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.6.0\n",
      "  Using cached gensim-3.6.0-cp39-cp39-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /Users/riccardopandolfi/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/riccardopandolfi/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.6.0) (1.9.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/riccardopandolfi/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.6.0) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/riccardopandolfi/opt/anaconda3/lib/python3.9/site-packages (from gensim==3.6.0) (1.21.5)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.1\n",
      "    Uninstalling gensim-4.3.1:\n",
      "      Successfully uninstalled gensim-4.3.1\n",
      "Successfully installed gensim-3.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# nlp lib\n",
    "import sys\n",
    "import nltk \n",
    "nltk.download('omw-1.4')\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize   \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import string, collections, unicodedata\n",
    "!pip install gensim==3.6.0\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec # library for text analysis\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b542f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two datasets\n",
    "donald_trump = pd.read_csv('hashtag_donaldtrump.csv', lineterminator='\\n')\n",
    "joe_biden = pd.read_csv('hashtag_joebiden.csv', lineterminator='\\n')\n",
    "\n",
    "# Add the column \"candidate\" to know who the tweet was referring to\n",
    "donald_trump.loc[:,'candidate'] = 'Donald Trump'\n",
    "joe_biden.loc[:,'candidate'] = 'Joe Biden'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263ae7d",
   "metadata": {},
   "source": [
    "###  Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1607a438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete the rows with missing values\n",
    "donald_trump=donald_trump.dropna()\n",
    "joe_biden=joe_biden.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c3d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the country name to have consistent data\n",
    "d = {\"United States of America\":\"United States\"}\n",
    "donald_trump['country'].replace(d, inplace=True)\n",
    "joe_biden['country'].replace(d, inplace=True)\n",
    "\n",
    "# Only consider tweets from the US\n",
    "donald_trump = donald_trump.loc[donald_trump['country'] == \"United States\"]\n",
    "joe_biden = joe_biden.loc[joe_biden['country'] == \"United States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd0d3ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/riccardopandolfi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/riccardopandolfi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Create functions to remove stop words and lemmatize\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean(text):\n",
    "    # Lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove special text in brackets ([chorus],[guitar],etc)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # Remove words containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    # Remove quotes\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    # Remove new line \\n \n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) \n",
    "    # Remove stop_word\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(text)\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_tag(text):\n",
    "    lemma=[]\n",
    "    for i,j in pos_tag(word_tokenize(text)) :\n",
    "        p=j[0].lower()\n",
    "        lm = ''\n",
    "        if p in ['j','n','v']:\n",
    "            if p == 'j':\n",
    "                p = 'a'\n",
    "            lm = wnl.lemmatize(i,p)\n",
    "            \n",
    "        else :\n",
    "            lm = wnl.lemmatize(i)\n",
    "        if len(lm) > 1:\n",
    "            lemma.append(lm)\n",
    "    return ' '.join(lemma)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "                \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a clean_tweet column \n",
    "donald_trump['cleaned_tweet'] = donald_trump['tweet'].apply(clean)\n",
    "donald_trump['cleaned_tweet'] = donald_trump['cleaned_tweet'].apply(lemmatize_tag)\n",
    "donald_trump['cleaned_tweet'] = donald_trump['cleaned_tweet'].apply(remove_emoji)\n",
    "\n",
    "joe_biden['cleaned_tweet'] = joe_biden['tweet'].apply(clean)\n",
    "joe_biden['cleaned_tweet'] = joe_biden['cleaned_tweet'].apply(lemmatize_tag)\n",
    "joe_biden['cleaned_tweet'] = joe_biden['cleaned_tweet'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df5292",
   "metadata": {},
   "source": [
    "### Setiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d82431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polarity neg/positive labels\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "donald_trump[\"scores\"] = donald_trump[\"cleaned_tweet\"].apply(lambda cleaned_tweet: sid.polarity_scores(cleaned_tweet))\n",
    "joe_biden[\"scores\"] = joe_biden[\"cleaned_tweet\"].apply(lambda cleaned_tweet: sid.polarity_scores(cleaned_tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0542fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add neg/positive compound and labels\n",
    "donald_trump[\"compound\"] = donald_trump[\"scores\"].apply(lambda score_dict: score_dict[\"compound\"])\n",
    "joe_biden[\"compound\"] = joe_biden[\"scores\"].apply(lambda score_dict: score_dict[\"compound\"])\n",
    "\n",
    "donald_trump[\"comp_score\"] = donald_trump[\"compound\"].apply(lambda c: \"pos\" if c >= 0.3 else \"neg\")\n",
    "joe_biden[\"comp_score\"] = joe_biden[\"compound\"].apply(lambda c: \"pos\" if c >= 0.3 else \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two datasets\n",
    "df = pd.concat([joe_biden,donald_trump])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cleaned_tweet.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063db95",
   "metadata": {},
   "source": [
    "### Exloratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9104af",
   "metadata": {},
   "source": [
    "#### Number of Tweets by Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the tweets by candidate and count the number of tweets\n",
    "tweet_counts = df.groupby('candidate')['tweet_id'].count()\n",
    "\n",
    "# create a bar chart of the tweet counts for each candidate using plotly graph objects\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=tweet_counts.index,\n",
    "    y=tweet_counts,\n",
    "    text=tweet_counts,\n",
    "    textposition='auto',\n",
    "    marker_color=['#E9141D', '#00308F']\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Number of Tweets by Candidate\",\n",
    "    xaxis_title=\"Candidate\",\n",
    "    yaxis_title=\"Number of Tweets\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b305d",
   "metadata": {},
   "source": [
    "#### Number of Tweets by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.query('(candidate == \"Joe Biden\") & (country == \"United States\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False)\n",
    "x = df.query('(candidate == \"Joe Biden\") & (country == \"United States\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False).index\n",
    "y2 = df.query('(candidate == \"Donald Trump\") & (country == \"United States\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False)\n",
    "x2 = df.query('(candidate == \"Donald Trump\") & (country == \"United States\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False).index\n",
    "fig = go.Figure([go.Bar(x=x, y=y, name='Joe Biden'),\n",
    "                 go.Bar(x=x2, y=y2, name='Donald Trump')])\n",
    "\n",
    "# Customize aspect\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Number of Tweets by Candidate and by State\",\n",
    "    xaxis_title=\"States\",\n",
    "    yaxis_title=\"Number of Tweets\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39806c35",
   "metadata": {},
   "source": [
    "#### Number of Tweets by Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number of tweets per day by candidate\n",
    "# Convert 'created_at' column to datetime type\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Create two dataframes for each candidate\n",
    "trump_df = df[df['candidate'] == 'Donald Trump']\n",
    "biden_df = df[df['candidate'] == 'Joe Biden']\n",
    "\n",
    "# Group tweets by date and count number of tweets per date\n",
    "trump_count = trump_df.groupby(pd.Grouper(key='created_at', freq='D')).size()\n",
    "biden_count = biden_df.groupby(pd.Grouper(key='created_at', freq='D')).size()\n",
    "\n",
    "# Create figure with two traces for each candidate\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=trump_count.index, y=trump_count, name='Donald Trump', marker_color='#E9141D'))\n",
    "fig.add_trace(go.Bar(x=biden_count.index, y=biden_count, name='Joe Biden', marker_color='#00308F'))\n",
    "\n",
    "# Update figure layout\n",
    "fig.update_layout(title=\"Number of Tweets Per Day And By Candidate\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Number of Tweets\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"))\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8600a",
   "metadata": {},
   "source": [
    "#### Positive Joe Biden Tweets by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fa600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import json\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/us-states.json'\n",
    "filename = 'us-states.json'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Filter for Joe Biden tweets and positive compound scores\n",
    "biden_pos = df[(df['candidate'] == 'Joe Biden') & (df['comp_score'] == 'pos')]\n",
    "\n",
    "# Group by state and count number of tweets\n",
    "biden_pos_counts = biden_pos.groupby('state_code').size().reset_index(name='count')\n",
    "\n",
    "# Load US states GeoJSON file\n",
    "with open('us-states.json') as f:\n",
    "    states = json.load(f)\n",
    "\n",
    "# Create choropleth map\n",
    "fig = px.choropleth(biden_pos_counts, \n",
    "                    locations='state_code', \n",
    "                    geojson=states, \n",
    "                    color='count',\n",
    "                    scope=\"usa\",\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                    labels={'count':'Number of Tweets'})\n",
    "fig.update_layout(title_text='Positive Joe Biden Tweets by State',  font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545e977",
   "metadata": {},
   "source": [
    "#### Number of Positive and Negative Tweets Before the Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for tweets created before 2020-11-04 00:00:00\n",
    "df_filtered = df[df['created_at'] < '2020-11-04 00:00:00']\n",
    "\n",
    "# Group by candidate and comp_score and count number of tweets\n",
    "counts = df_filtered.groupby(['candidate', 'comp_score']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the data to have positive and negative counts as separate columns\n",
    "counts_pivot = counts.pivot(index='candidate', columns='comp_score', values='count').reset_index()\n",
    "\n",
    "# Rename columns\n",
    "counts_pivot = counts_pivot.rename(columns={'pos': 'Positive Tweets', 'neg': 'Negative Tweets'})\n",
    "\n",
    "# Create bar chart\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=counts_pivot['candidate'], y=counts_pivot['Positive Tweets'],\n",
    "                     name='Positive Tweets', marker_color='#E4A0F7'))\n",
    "fig.add_trace(go.Bar(x=counts_pivot['candidate'], y=counts_pivot['Negative Tweets'],\n",
    "                     name='Negative Tweets', marker_color='#414141'))\n",
    "fig.update_layout(title='Number of Positive and Negative Tweets Before The Election',\n",
    "                  xaxis_title='Candidate', yaxis_title='Number of Tweets', font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76c174",
   "metadata": {},
   "source": [
    "#### Sentiment Change Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce23e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the date to the nearest hour\n",
    "df['round_date'] = pd.to_datetime(df['created_at']).dt.floor('D')\n",
    "\n",
    "# Group the tweets by date and candidate\n",
    "import pandas as pd\n",
    "grouped = df.groupby(['round_date', 'candidate'])\n",
    "\n",
    "# Calculate the sentiment\n",
    "sentiment = grouped['compound'].mean().unstack()\n",
    "\n",
    "# Plot the data\n",
    "sentiment.plot(color=['red', 'blue'])\n",
    "plt.title('Sentiment over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9533a",
   "metadata": {},
   "source": [
    "#### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for word clouds\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "def word_cloud(wd_list):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    all_words = ' '.join([text for text in wd_list])\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                         stopwords= stopwords,\n",
    "                         width = 1600,height=800,\n",
    "                         random_state=1,\n",
    "                         colormap='jet',\n",
    "                         max_words=50,\n",
    "                         max_font_size=200).generate(all_words)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordcloud)\n",
    "\n",
    "# Donald Trump word cloud\n",
    "word_cloud(donald_trump['cleaned_tweet'][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joe Biden word cloud\n",
    "word_cloud(joe_biden['cleaned_tweet'][:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04680bae",
   "metadata": {},
   "source": [
    "### Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cd8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.cleaned_tweet.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5030fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c476465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only once\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsVec = [nltk.word_tokenize(cleaned_tweet) for cleaned_tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tweetsVec, min_count = 4, size=15, window=3, workers = 4, seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def5401",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_biden = model.wv.most_similar('joebiden', topn=15)\n",
    "x = [i for i,j in similar_biden]\n",
    "y = [j for i,j in similar_biden]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.barplot(x=x, y=y, color=\"blue\", saturation=.5, ax=ax)\n",
    "ax.set_title('Closest words to Joe Biden')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "ax.set_ylim(np.min(y),np.max(y))\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_ylabel('Proportion of similarities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_trump = model.wv.most_similar('donaldtrump',topn=15)\n",
    "x = [i for i,j in similar_trump]\n",
    "y = [j for i,j in similar_trump]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.barplot(x=x, y=y, color=\"red\", saturation=.5, ax=ax)\n",
    "ax.set_title('Closest words to Donald Trump')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "ax.set_ylim(np.min(y),np.max(y))\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_ylabel('Proportion of similarities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f18fa5",
   "metadata": {},
   "source": [
    "### Topic Modeling - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fba31d",
   "metadata": {},
   "source": [
    "#### On tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "df_cv = cv.fit_transform(df['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "LDA.fit(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1995a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top words per topic\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10218efb",
   "metadata": {},
   "source": [
    "#### On users' descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a cleaned_user_description column \n",
    "df['cleaned_user_description'] = df['user_description'].apply(clean)\n",
    "df['cleaned_user_description'] = df['cleaned_user_description'].apply(lemmatize_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "df_cv = cv.fit_transform(df['cleaned_user_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9843af",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "LDA.fit(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df307b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a170ce3",
   "metadata": {},
   "source": [
    "### Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the dataset for the model\n",
    "\n",
    "df = df.loc[:, ['cleaned_tweet', 'comp_score']]\n",
    "df.loc[df['comp_score'].isnull(), 'comp_score'] = 0\n",
    "df.loc[df['comp_score'] == 'pos', 'comp_score'] = 1\n",
    "df.loc[df['comp_score'] == 'neg', 'comp_score'] = 0\n",
    "df = df.dropna()\n",
    "df['comp_score']= df['comp_score'].astype('int')\n",
    "df.dtypes\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c49412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test splitting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"cleaned_tweet\"]  # this time we want to look at the text\n",
    "y = df[\"comp_score\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFidVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train= vectorizer.fit_transform(\n",
    "    X_train\n",
    ")  # remember to use the original X_train set\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Logistic Regression and Decision Tree models and see which one performs better so we can tune its parameters\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Create and train different classification models\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Training {name} model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate metrics on the training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, average=\"macro\")\n",
    "    train_recall = recall_score(y_train, y_train_pred, average=\"macro\")\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "\n",
    "    # Transform X_test using the vectorizer\n",
    "    X_test_transformed = vectorizer.transform(X_test)\n",
    "    \n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    \n",
    "    # Evaluate the model performance using different metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    # Print the performance metrics for the training set\n",
    "    print(f\"{name} model training set results:\")\n",
    "    print(f\"Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"Precision: {train_precision:.2f}\")\n",
    "    print(f\"Recall: {train_recall:.2f}\")\n",
    "    print(f\"F1 Score: {train_f1:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Print the performance metrics for the test set\n",
    "    print(f\"{name} model test set results:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250afd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(logreg, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores and their mean\n",
    "print(\"Logistic Regression cross-validation accuracy scores:\")\n",
    "print(cv_scores)\n",
    "print(\"Mean cross-validation accuracy:\", cv_scores.mean())\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Calculate metrics on the training set\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average=\"macro\")\n",
    "train_recall = recall_score(y_train, y_train_pred, average=\"macro\")\n",
    "train_f1 = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "\n",
    "# Transform X_test using the vectorizer\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions using the test set\n",
    "y_pred = logreg.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model performance using different metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "# Print the performance metrics for the training set\n",
    "print(f\"Logistic Regression model training set results:\")\n",
    "print(f\"Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Precision: {train_precision:.2f}\")\n",
    "print(f\"Recall: {train_recall:.2f}\")\n",
    "print(f\"F1 Score: {train_f1:.2f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print the performance metrics for the test set\n",
    "print(f\"Logistic Regression model test set results:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improve the model with GridSearch\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load and preprocess the dataset (assuming this part is already done)\n",
    "\n",
    "# Split the dataset into training and test sets (assuming this part is already done)\n",
    "\n",
    "# Vectorize the text data (assuming this part is already done)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create the GridSearchCV object with 2 folds\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters on the entire training set\n",
    "best_logreg = grid_search.best_estimator_\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Calculate metrics on the training set\n",
    "y_train_pred = best_logreg.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average=\"macro\")\n",
    "train_recall = recall_score(y_train, y_train_pred, average=\"macro\")\n",
    "train_f1 = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "\n",
    "# Transform X_test using the vectorizer\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions using the test set\n",
    "y_pred = best_logreg.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model performance using different metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "# Print the performance metrics for the training set\n",
    "print(f\"Logistic Regression model training set results:\")\n",
    "print(f\"Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Precision: {train_precision:.2f}\")\n",
    "print(f\"Recall: {train_recall:.2f}\")\n",
    "print(f\"F1 Score: {train_f1:.2f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print the performance metrics for the test set\n",
    "print(f\"Logistic Regression model test set results:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create a confusion matrix for the test set predictions\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "\n",
    "# Plot the ROC curve\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(best_logreg, X_test_transformed, y_test)\n",
    "plt.title('ROC Curve')\n",
    "plt.savefig('roc_curve.png')\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "plot_precision_recall_curve(best_logreg, X_test_transformed, y_test)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "\n",
    "# Plot the feature importances\n",
    "feature_importances = best_logreg.coef_.ravel()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Important Features')\n",
    "plt.savefig('feature_importances.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5669be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Them\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create a confusion matrix for the test set predictions\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "\n",
    "# Plot the ROC curve\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(best_logreg, X_test_transformed, y_test)\n",
    "plt.title('ROC Curve')\n",
    "plt.savefig('roc_curve.png')\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "plot_precision_recall_curve(best_logreg, X_test_transformed, y_test)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "\n",
    "# Plot the feature importances\n",
    "feature_importances = best_logreg.coef_.ravel()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df['Class'] = ['Negative' if imp < 0 else 'Positive' for imp in importance_df['Importance']]\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature', hue='Class')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Important Features')\n",
    "plt.savefig('feature_importances.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94778b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add distinct feature (words) importance for Positive and Negative tweets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create a confusion matrix for the test set predictions\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "\n",
    "# Plot the ROC curve\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(best_logreg, X_test_transformed, y_test)\n",
    "plt.title('ROC Curve')\n",
    "plt.savefig('roc_curve.png')\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "plot_precision_recall_curve(best_logreg, X_test_transformed, y_test)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "\n",
    "# Create a dataframe for the top 20 most relevant features for positive target variable\n",
    "pos_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "pos_df['Target'] = 'Positive'\n",
    "pos_df = pos_df.sort_values(['Target', 'Importance'], ascending=[True, False])[:20]\n",
    "\n",
    "# Create a dataframe for the top 20 most relevant features for negative target variable\n",
    "neg_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "neg_df['Target'] = 'Negative'\n",
    "neg_df = neg_df.sort_values(['Target', 'Importance'], ascending=[True, True])[:20]\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df = pd.concat([pos_df, neg_df])\n",
    "\n",
    "# Plot the feature importances for positive and negative target variables separately\n",
    "g = sns.catplot(data=df, x='Importance', y='Feature', hue='Target', kind='bar', height=10, aspect=0.8)\n",
    "g.ax.set_xlabel('Importance')\n",
    "g.ax.set_ylabel('Feature')\n",
    "g.ax.set_title('Top 20 Important Features')\n",
    "plt.savefig('feature_importances.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
